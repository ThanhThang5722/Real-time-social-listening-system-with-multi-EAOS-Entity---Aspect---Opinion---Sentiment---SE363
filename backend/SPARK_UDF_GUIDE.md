# üöÄ PySpark PandasUDF for EAOS Model

## T·ªïng quan

Model Multi-EAOS ƒë√£ ƒë∆∞·ª£c t√≠ch h·ª£p v·ªõi **PySpark PandasUDF** ƒë·ªÉ x·ª≠ l√Ω h√†ng tri·ªáu comments song song tr√™n Spark cluster.

### ‚ö° Performance
- **Without UDF (Python loop):** ~1 comment/second
- **With PandasUDF:** ~100-1000 comments/second (depending on cluster size)
- **Speedup:** 10-100x faster!

---

## üìÅ Files

```
application/backend/
‚îú‚îÄ‚îÄ ml/
‚îÇ   ‚îú‚îÄ‚îÄ eaos_model.py         # Model class + Inference (UPDATED ‚úÖ)
‚îÇ   ‚îú‚îÄ‚îÄ spark_inference.py    # PandasUDF + Spark job
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ models/checkpoints/
‚îÇ   ‚îú‚îÄ‚îÄ latest_checkpoint.pth # Model weights (1.7GB)
‚îÇ   ‚îî‚îÄ‚îÄ config.json           # Configuration
‚îî‚îÄ‚îÄ test_spark_udf.py         # Test script
```

---

## üîß Architecture Update (FIXED)

### ‚ùå **Before (WRONG):**
```python
# Old ml/eaos_model.py used BiLSTM
self.lstm = nn.LSTM(...)  # ‚Üê Incompatible with checkpoint!
```

### ‚úÖ **After (CORRECT):**
```python
# Updated ml/eaos_model.py uses Transformer
self.transformer = nn.TransformerEncoder(...)  # ‚Üê Matches checkpoint!
```

### üîÑ **Model Loading Updated:**
```python
# Now loads from latest_checkpoint.pth instead of model.pth
checkpoint = torch.load("latest_checkpoint.pth")
model.load_state_dict(checkpoint['model_state_dict'])
```

---

## üß™ Testing

### Quick Test (Local Mode)
```bash
cd application/backend
python test_spark_udf.py
```

**Expected Output:**
```
======================================================================
TEST 1: Model Loading
======================================================================
‚úÖ Model loaded successfully!
   Model type: MultiEAOSModel
   Config: vinai/phobert-base
   Best epoch: 110

======================================================================
TEST 2: Inference
======================================================================
‚úÖ Inference service created
‚úÖ Predictions: 1 labels found

   Label 1:
      Entity: ch∆∞∆°ng tr√¨nh
      Aspect: K·ªãch b·∫£n
      Opinion: r·∫•t hay
      Sentiment: t√≠ch c·ª±c
      Confidence: 0.982

======================================================================
TEST 3: Spark PandasUDF
======================================================================
‚úÖ Spark session created (local mode)
‚úÖ PandasUDF created
‚úÖ Spark PandasUDF test passed!
```

---

## üöÄ Usage

### Option 1: Standalone Spark Job

```bash
python -m ml.spark_inference \
  --model-dir ./models/checkpoints \
  --input ./data/comments.json \
  --output ./results/predictions.json \
  --mode file
```

**Input Format (JSON):**
```json
{"id": "1", "text": "Ch∆∞∆°ng tr√¨nh r·∫•t hay!"}
{"id": "2", "text": "MC d·∫´n t·ªët qu√°"}
```

**Output Format:**
```json
{
  "id": "1",
  "text": "Ch∆∞∆°ng tr√¨nh r·∫•t hay!",
  "predictions": "[{\"entity\":\"ch∆∞∆°ng tr√¨nh\",\"aspect\":\"K·ªãch b·∫£n\",\"opinion\":\"r·∫•t hay\",\"sentiment\":\"t√≠ch c·ª±c\",\"confidence\":0.95}]"
}
```

### Option 2: Python Script

```python
from ml.spark_inference import run_spark_job

# Run with sample data (console output)
run_spark_job(
    model_dir="./models/checkpoints",
    mode="console"
)

# Run with file input/output
run_spark_job(
    model_dir="./models/checkpoints",
    input_path="./data/comments.json",
    output_path="./results/predictions.json",
    mode="file"
)

# Return as Pandas DataFrame
df = run_spark_job(
    model_dir="./models/checkpoints",
    input_path="./data/comments.json",
    mode="memory"
)
print(df.head())
```

### Option 3: Custom Spark Application

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from ml.spark_inference import create_eaos_udf

# Create Spark session
spark = SparkSession.builder \
    .appName("EAOS-Production") \
    .master("spark://localhost:7077") \
    .config("spark.executor.memory", "8g") \
    .config("spark.executor.cores", "4") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
    .getOrCreate()

# Load data
df = spark.read.json("hdfs://data/comments.json")

# Create UDF
eaos_udf = create_eaos_udf("./models/checkpoints")

# Apply UDF (distributed processing!)
result_df = df.withColumn("predictions", eaos_udf(col("text")))

# Save results
result_df.write.mode("overwrite").parquet("hdfs://results/predictions.parquet")

spark.stop()
```

---

## ‚öôÔ∏è Configuration

### Spark Settings (spark_inference.py:119-125)

```python
spark = SparkSession.builder \
    .appName("EAOS-Inference") \
    .master("spark://localhost:7077")  # Change to your Spark master
    .config("spark.executor.memory", "4g")  # Memory per executor
    .config("spark.driver.memory", "2g")    # Driver memory
    .config("spark.sql.execution.arrow.pyspark.enabled", "true")  # Enable Arrow
    .getOrCreate()
```

### Model Configuration

Model ƒë∆∞·ª£c load m·ªôt l·∫ßn tr√™n **m·ªói Spark executor** (not per row):

```python
# Global model (loaded once per worker)
_model = None

def initialize_model(model_dir):
    global _model
    if _model is None:
        _model = load_model(model_dir, device='cpu')  # Use CPU on workers
    return _model
```

---

## üìä Performance Tuning

### 1. Executor Configuration
```python
# TƒÉng s·ªë executors v√† cores
.config("spark.executor.instances", "10")
.config("spark.executor.cores", "4")
.config("spark.executor.memory", "8g")
```

### 2. Batch Size
PandasUDF t·ª± ƒë·ªông x·ª≠ l√Ω theo batch (vectorized). ƒêi·ªÅu ch·ªânh partition size:
```python
df = df.repartition(100)  # Chia data th√†nh 100 partitions
```

### 3. GPU Support
N·∫øu executors c√≥ GPU:
```python
def initialize_model(model_dir):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    return load_model(model_dir, device=device)
```

### 4. Confidence Threshold
Gi·∫£m threshold ƒë·ªÉ c√≥ nhi·ªÅu predictions h∆°n (nh∆∞ng √≠t ch√≠nh x√°c h∆°n):
```python
inference = EAOSInference(model, tokenizer, config, confidence_threshold=0.2)
```

---

## üîç Monitoring

### Spark UI
Khi job ƒëang ch·∫°y, truy c·∫≠p:
```
http://localhost:4040
```

Xem:
- Tasks progress
- Stage timeline
- Executor metrics
- Storage usage

### Logs
```python
spark.sparkContext.setLogLevel("INFO")  # Change to DEBUG for more details
```

---

## üìà Scalability

### Data Size vs. Processing Time

| Comments | Executors | Cores | Time (estimate) |
|----------|-----------|-------|-----------------|
| 10K      | 1         | 2     | ~2 minutes      |
| 100K     | 5         | 4     | ~5 minutes      |
| 1M       | 10        | 8     | ~15 minutes     |
| 10M      | 20        | 8     | ~2 hours        |

*Assuming: 2s per comment (CPU), batch processing with PandasUDF*

---

## üêõ Troubleshooting

### Error: "No module named 'ml'"
```bash
# Add to PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:/path/to/backend"
```

### Error: "Checkpoint not found"
```bash
# Verify files exist
ls -lh models/checkpoints/
# Should see: latest_checkpoint.pth, config.json
```

### Error: "Architecture mismatch"
**Solution:** This was fixed! The model now uses Transformer instead of BiLSTM.

If you still see this error:
1. Check you're using updated `ml/eaos_model.py`
2. Verify checkpoint is from notebook (not old BiLSTM model)

### Error: "Out of memory"
**Solutions:**
1. Reduce executor memory
2. Increase repartitions: `df.repartition(200)`
3. Use CPU instead of GPU on workers
4. Process smaller batches

### Slow Performance
**Solutions:**
1. Enable Arrow: `spark.sql.execution.arrow.pyspark.enabled = true`
2. Increase executors and cores
3. Use faster storage (SSD, HDFS instead of network mount)
4. Cache intermediate results: `df.cache()`

---

## üîó Integration with Backend

### FastAPI + Spark for Batch Processing

```python
# api/routes.py
from ml.spark_inference import run_spark_job

@router.post("/batch/process-large")
async def process_large_batch(file_path: str):
    """Process millions of comments with Spark"""
    try:
        # Run Spark job asynchronously
        result_df = run_spark_job(
            model_dir="./models/checkpoints",
            input_path=file_path,
            mode="memory"
        )

        return {
            "status": "success",
            "total_processed": len(result_df),
            "sample": result_df.head(5).to_dict('records')
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## üìù Notes

1. **Model Loading**
   - Model is loaded **once per executor**, not per row
   - This amortizes the loading cost across thousands of predictions

2. **Device Selection**
   - Default: CPU (compatible with all executors)
   - For GPU: Ensure all executors have CUDA-compatible GPUs

3. **Confidence Threshold**
   - Lower threshold ‚Üí More predictions (but less accurate)
   - Higher threshold ‚Üí Fewer predictions (but more accurate)
   - Default: 0.5 (recommended: 0.3-0.7)

4. **Arrow Optimization**
   - **Must enable** for PandasUDF to work efficiently
   - Without Arrow: 10x slower serialization

---

## ‚úÖ Checklist

Before running in production:

- [ ] Model checkpoint exists and loads successfully
- [ ] Test script passes all tests
- [ ] Spark cluster is configured and accessible
- [ ] Input data format is correct (JSON with "text" field)
- [ ] Output path is writable
- [ ] Executors have enough memory (4GB+ recommended)
- [ ] Arrow is enabled in Spark config
- [ ] Confidence threshold is tuned for your use case

---

## üÜò Support

**Common Issues:**
1. Import errors ‚Üí Check PYTHONPATH
2. Memory errors ‚Üí Reduce executor memory or increase repartitions
3. Slow performance ‚Üí Enable Arrow, increase cores
4. Model mismatch ‚Üí Use updated ml/eaos_model.py (Transformer, not BiLSTM)

**Contact:**
- Check logs in Spark UI: http://localhost:4040
- Review test output: `python test_spark_udf.py`
