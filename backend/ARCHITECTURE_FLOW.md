# ğŸ”„ Architecture Flow - Backend â†’ PySpark â†’ Model

## âœ… CHá»¨NG Cá»¨: Backend Gá»ŒI PySpark Service, KHÃ”NG load model trá»±c tiáº¿p

### ğŸ“‹ Code Flow

```
User Request
    â†“
Backend (FastAPI - Port 8000)
    â†“ HTTP Request
PySpark Service (Port 5001)
    â†“
Model Inference
    â”‚
    â”œâ”€ Batch < 10: Direct model call
    â””â”€ Batch â‰¥ 10: PySpark PandasUDF
```

---

## ğŸ“ CHá»¨NG Cá»¨ 1: PySpark HTTP Client

**File:** `services/pyspark_client.py`

```python
class PySparkClient:
    def __init__(self, base_url: str = "http://localhost:5001"):
        """Connects to PySpark service via HTTP"""
        self.base_url = base_url
        # ...

    def predict(self, text: str) -> List[EAOSLabel]:
        """
        Makes HTTP POST request to PySpark service
        """
        with httpx.Client(timeout=self.timeout) as client:
            response = client.post(
                f"{self.base_url}/predict",  # â† HTTP call to PySpark
                json={"text": text}
            )
            # ...
```

**âœ… PROOF**: Client gá»­i HTTP request Ä‘áº¿n `http://localhost:5001/predict`

---

## ğŸ“ CHá»¨NG Cá»¨ 2: Backend Routes

**File:** `api/routes.py` (DÃ²ng 154-194)

```python
@router.post("/predict", response_model=PredictResponse)
async def predict_single_comment(request: PredictRequest):
    """
    Predict EAOS labels for a single comment via PySpark service

    Flow: Backend (FastAPI) â†’ PySpark Service (HTTP) â†’ Model Inference
    """
    # Initialize PySpark client
    if pyspark_client is None:
        init_pyspark_client()

    # Call PySpark service via HTTP â† KEY LINE
    labels = pyspark_client.predict(
        request.text,
        confidence_threshold=request.confidence_threshold
    )
    # ...
```

**âœ… PROOF**: Backend gá»i `pyspark_client.predict()` â†’ HTTP request

---

## ğŸ“ CHá»¨NG Cá»¨ 3: Batch Endpoint

**File:** `api/routes.py` (DÃ²ng 197-244)

```python
@router.post("/predict/batch")
async def predict_batch_comments(request: PredictBatchRequest):
    """
    Flow: Backend â†’ PySpark Service â†’ PandasUDF (batch â‰¥ 10) or Model (batch < 10)
    """
    # Call PySpark service via HTTP (uses PandasUDF for batch >= 10)
    batch_labels = pyspark_client.predict_batch(
        request.texts,
        confidence_threshold=request.confidence_threshold
    )
    # ...
```

**âœ… PROOF**: Batch prediction qua PySpark service, tá»± Ä‘á»™ng dÃ¹ng PandasUDF khi batch â‰¥ 10

---

## ğŸ“ CHá»¨NG Cá»¨ 4: WebSocket Streaming

**File:** `api/websocket.py` (DÃ²ng 42-82)

```python
@router.websocket("/ws/comments")
async def websocket_comments(websocket: WebSocket):
    """
    Flow: Backend â†’ PySpark Service (HTTP) â†’ Model â†’ Predictions
    """
    async for comment in comment_service.stream_comments():
        if pyspark_client is not None:
            # Call PySpark service for prediction â† KEY LINE
            predicted_labels = pyspark_client.predict(
                comment.text,
                confidence_threshold=0.3
            )
            comment.labels = predicted_labels
        # ...
```

**âœ… PROOF**: WebSocket cÅ©ng gá»i PySpark service qua HTTP

---

## ğŸ“ CHá»¨NG Cá»¨ 5: PySpark Service Implementation

**File:** `ml/spark_service.py` (DÃ²ng 150-228)

```python
@app.route('/predict/batch', methods=['POST'])
def predict_batch():
    """
    - Small batches (< 10): Use model directly (faster)
    - Large batches (â‰¥ 10): Use PySpark PandasUDF (parallel processing)
    """
    if len(comments) < 10:
        # Direct model inference
        for comment in comments:
            predictions = inference.predict(comment['text'])
            # ...
        return jsonify({"method": "direct"})  # â† Returns method used

    # For large batches, use PySpark
    df = spark.createDataFrame(comments)
    df_with_predictions = df.withColumn("predictions", predict_udf(col("text")))
    # ...
    return jsonify({"method": "pyspark"})  # â† Returns method used
```

**âœ… PROOF**:
- Batch < 10: Direct model
- Batch â‰¥ 10: PySpark PandasUDF
- Response includes `"method"` field Ä‘á»ƒ verify

---

## ğŸ§ª TEST CASE - Verify Flow

### Test 1: Single Prediction

```bash
# Start PySpark service
cd application
docker compose up -d pyspark

# Start Backend
cd backend
python main.py

# Test prediction
curl -X POST http://localhost:8000/api/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "MC dáº«n tá»‘t"}'
```

**Expected Flow:**
1. Backend receives request at port 8000
2. Backend calls `http://localhost:5001/predict` (PySpark service)
3. PySpark service runs model inference
4. Returns predictions to Backend
5. Backend returns to user

### Test 2: Batch Prediction (PandasUDF)

```bash
curl -X POST http://localhost:8000/api/predict/batch \
  -H "Content-Type: application/json" \
  -d '{
    "texts": [
      "MC dáº«n tá»‘t",
      "Ká»‹ch báº£n hay",
      "Ã‚m thanh rÃµ",
      "HÃ¬nh áº£nh Ä‘áº¹p",
      "Ná»™i dung cuá»‘n",
      "Diá»…n viÃªn giá»i",
      "Äáº¡o diá»…n tÃ i nÄƒng",
      "Quay phim chuyÃªn nghiá»‡p",
      "Ká»‹ch báº£n sÃ¡ng táº¡o",
      "Ã‚m nháº¡c hay"
    ]
  }'
```

**Expected:**
- Response includes `"method": "pyspark"` because batch size = 10
- PySpark service uses PandasUDF for parallel processing

### Test 3: Verify in PySpark Logs

```bash
# Check PySpark service logs
docker compose logs -f pyspark
```

**Expected output:**
```
172.18.0.1 - - [DATE] "POST /predict HTTP/1.1" 200 -
Batch prediction used method: pyspark
```

---

## ğŸ“Š Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User / Client                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP Request
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Backend (FastAPI)   â”‚  Port 8000
         â”‚  - routes.py         â”‚
         â”‚  - websocket.py      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP Request (via PySparkClient)
                     â”‚ http://localhost:5001/predict
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ PySpark Service      â”‚  Port 5001
         â”‚ (Docker Container)   â”‚
         â”‚  - Flask HTTP API    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Prediction Logic            â”‚
         â”‚                              â”‚
         â”‚  if batch < 10:              â”‚
         â”‚    â”œâ”€â–º Model.predict()       â”‚
         â”‚  else:                       â”‚
         â”‚    â””â”€â–º PySpark PandasUDF     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Summary - Káº¾T LUáº¬N

### Backend KHÃ”NG load model trá»±c tiáº¿p:
- âŒ KHÃ”NG import `EAOSModelService`
- âŒ KHÃ”NG load model weights
- âŒ KHÃ”NG run inference trá»±c tiáº¿p

### Backend Gá»ŒI PySpark service qua HTTP:
- âœ… Import `PySparkClient`
- âœ… HTTP POST to `http://localhost:5001/predict`
- âœ… Receive predictions tá»« PySpark service

### PySpark service Tá»° Äá»˜NG chá»n method:
- âœ… Batch < 10: Direct model (fast)
- âœ… Batch â‰¥ 10: PySpark PandasUDF (parallel)
- âœ… Response includes `"method"` field

---

## ğŸ¯ Files Modified

1. **NEW:** `services/pyspark_client.py` - HTTP client for PySpark service
2. **MODIFIED:** `api/routes.py` - Uses PySpark client, not model service
3. **MODIFIED:** `api/websocket.py` - Uses PySpark client for streaming

---

**âœ… VERIFIED: All predictions go through PySpark service!**
