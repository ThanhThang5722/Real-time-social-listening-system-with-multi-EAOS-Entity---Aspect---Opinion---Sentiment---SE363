# ğŸš€ Apache Airflow Setup - Pipeline Orchestration

## ğŸ“‹ Tá»•ng quan

Airflow quáº£n lÃ½ vÃ  tá»± Ä‘á»™ng hÃ³a cÃ¡c pipeline:
- âœ… **Model Retraining**: Train láº¡i model trÃªn Spark (weekly)
- âœ… **Batch Prediction**: Predict batch comments (daily)
- âœ… **Scheduling**: Tá»± Ä‘á»™ng cháº¡y theo lá»‹ch
- âœ… **Retry**: Tá»± Ä‘á»™ng retry khi failed
- âœ… **Logging**: Log chi tiáº¿t má»i bÆ°á»›c
- âœ… **Monitoring**: Web UI Ä‘á»ƒ theo dÃµi

---

## ğŸ—ï¸ Architecture vá»›i Airflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Airflow Web UI (Port 8080)                  â”‚
â”‚            admin / admin - Manage & Monitor DAGs            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚  Webserver  â”‚      â”‚  Scheduler  â”‚
    â”‚  (UI/API)   â”‚      â”‚  (Executor) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   PostgreSQL            â”‚
                         â”‚   (Metadata DB)         â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DAGs (Workflows):

1. model_retraining (Weekly - Sunday 2 AM)
   â”œâ”€â–º Extract data from MongoDB
   â”œâ”€â–º Check data quality
   â”œâ”€â–º Prepare training data
   â”œâ”€â–º Train model on Spark â† KEY TASK
   â”œâ”€â–º Evaluate model
   â”œâ”€â–º Deploy if better
   â””â”€â–º Notify completion

2. batch_prediction (Daily - 3 AM)
   â”œâ”€â–º Fetch unlabeled comments from MongoDB
   â”œâ”€â–º Call PySpark service (/predict/batch)
   â”œâ”€â–º Save predictions to MongoDB
   â”œâ”€â–º Generate analytics report
   â””â”€â–º Cleanup

Flow: Airflow â†’ PySpark Service â†’ Spark â†’ Model â†’ Results
```

---

## ğŸš€ Quick Start

### 1. Build Airflow Services

```bash
cd application

# Build Airflow image
docker compose build airflow-webserver airflow-scheduler airflow-init

# Start all services including Airflow
docker compose up -d
```

### 2. Access Airflow Web UI

Open browser: **http://localhost:8080**

**Login:**
- Username: `admin`
- Password: `admin`

### 3. Verify DAGs

You should see 2 DAGs:
1. `model_retraining` - Train model on Spark
2. `batch_prediction` - Batch predictions

---

## ğŸ“ DAG 1: Model Retraining

**File:** `airflow/dags/model_retraining_dag.py`

**Schedule:** Every Sunday at 2 AM

**Tasks:**
```
extract_data â†’ check_quality â†’ prepare_data â†’ train_model â†’ evaluate â†’ deploy â†’ notify
```

### Task Details:

#### 1. Extract Data
- Pulls comments with labels from MongoDB
- Saves to `/opt/Stage2/training_data.json`

#### 2. Check Quality
- Validates minimum 100 samples
- Checks label distribution

#### 3. Prepare Data
- Converts to training format
- Splits 80/10/10 (train/val/test)

#### 4. **Train Model on Spark** â† MAIN TASK
- Runs training script: `/opt/Stage2/train_model.py`
- Uses Spark for distributed training
- Saves checkpoint to `/opt/Stage2/latest_checkpoint.pth`

#### 5. Evaluate
- Calculates validation loss
- Generates metrics (F1, precision, recall)

#### 6. Deploy if Better
- Compares new_val_loss vs current_val_loss
- If better: Deploy to `/opt/backend/models/checkpoints/`
- If not: Skip deployment

#### 7. Notify
- Sends completion notification
- Reports deployment status

### Manual Trigger:

```bash
# Via Airflow CLI in container
docker exec -it tv-analytics-airflow-scheduler \
    airflow dags trigger model_retraining

# Or click "Trigger DAG" button in Web UI
```

---

## ğŸ“ DAG 2: Batch Prediction

**File:** `airflow/dags/batch_prediction_dag.py`

**Schedule:** Daily at 3 AM

**Tasks:**
```
fetch_comments â†’ predict_batch â†’ save_predictions â†’ generate_report â†’ cleanup
```

### Task Details:

#### 1. Fetch Comments
- Queries MongoDB for unlabeled comments
- Limit: 1000 per run

#### 2. Predict Batch
- Sends to PySpark service: `http://pyspark:5001/predict/batch`
- Auto uses PandasUDF if batch >= 10

#### 3. Save Predictions
- Updates MongoDB documents with predictions
- Adds `predicted_at` timestamp

#### 4. Generate Report
- Sentiment distribution
- Top entities
- Top aspects

#### 5. Cleanup
- Removes temporary files

### Manual Trigger:

```bash
# Via Airflow CLI
docker exec -it tv-analytics-airflow-scheduler \
    airflow dags trigger batch_prediction

# Or use Web UI
```

---

## ğŸ”§ Configuration

### Environment Variables

Set in `docker-compose.yml`:

```yaml
environment:
  # PySpark service URL
  - PYSPARK_SERVICE_URL=http://pyspark:5001

  # Backend URL
  - BACKEND_URL=http://host.docker.internal:8000

  # Airflow database
  - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
```

### Change Schedule

Edit DAG file:

```python
# model_retraining_dag.py
dag = DAG(
    'model_retraining',
    schedule_interval='0 2 * * 0',  # â† Change here (Cron format)
    # ...
)
```

**Cron Examples:**
- `0 2 * * 0` - Every Sunday at 2 AM
- `0 3 * * *` - Every day at 3 AM
- `0 */6 * * *` - Every 6 hours
- `@daily` - Daily at midnight
- `@weekly` - Weekly on Sunday at midnight
- `None` - Manual trigger only

---

## ğŸ“Š Monitoring & Logs

### View DAG Runs

1. Open Airflow UI: http://localhost:8080
2. Click on DAG name
3. See run history, success/failure

### View Task Logs

1. Click on task in DAG graph
2. Click "Log" button
3. See detailed execution logs

### Via Command Line

```bash
# List DAG runs
docker exec -it tv-analytics-airflow-scheduler \
    airflow dags list-runs -d model_retraining

# View task logs
docker exec -it tv-analytics-airflow-scheduler \
    airflow tasks logs model_retraining train_model 2025-01-01
```

---

## ğŸ› Troubleshooting

### Issue: DAGs not appearing

**Check:**
```bash
# List all DAGs
docker exec -it tv-analytics-airflow-scheduler airflow dags list

# Check for import errors
docker exec -it tv-analytics-airflow-scheduler airflow dags list-import-errors
```

**Fix:**
- Verify DAG files in `airflow/dags/`
- Check Python syntax errors
- Restart scheduler: `docker compose restart airflow-scheduler`

### Issue: Task failed

**Check logs:**
```bash
docker compose logs airflow-scheduler
```

**Retry task:**
- Click "Clear" button in Airflow UI
- Task will retry automatically

### Issue: Cannot connect to PySpark service

**Verify:**
```bash
# Test from Airflow container
docker exec -it tv-analytics-airflow-scheduler \
    curl http://pyspark:5001/health
```

**Expected:**
```json
{"model": true, "spark": true, "status": "healthy"}
```

---

## ğŸ“ Directory Structure

```
application/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ Dockerfile              # Airflow image
â”‚   â”œâ”€â”€ requirements.txt        # Python deps
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ model_retraining_dag.py     # Train DAG
â”‚   â”‚   â””â”€â”€ batch_prediction_dag.py     # Prediction DAG
â”‚   â”œâ”€â”€ logs/                   # Task logs
â”‚   â””â”€â”€ plugins/                # Custom plugins
â”‚
â”œâ”€â”€ docker-compose.yml          # Airflow services added
â”‚
â””â”€â”€ Stage2/
    â”œâ”€â”€ train_model.py          # Training script (to be created)
    â”œâ”€â”€ training_data.json      # Extracted data
    â”œâ”€â”€ train_data.json         # Training set
    â”œâ”€â”€ val_data.json           # Validation set
    â””â”€â”€ latest_checkpoint.pth   # Trained model
```

---

## ğŸ”„ Complete Training Pipeline Flow

```
1. Scheduled (Sunday 2 AM) or Manual Trigger
   â†“
2. Airflow Scheduler picks up DAG
   â†“
3. Extract Data Task
   MongoDB â†’ training_data.json
   â†“
4. Prepare Data Task
   Split â†’ train/val/test sets
   â†“
5. Train Model Task
   python /opt/Stage2/train_model.py
   â”œâ”€â–º Load train_data.json
   â”œâ”€â–º Initialize model (PhoBERT + Transformer)
   â”œâ”€â–º Spark distributed training
   â”œâ”€â–º Save checkpoint
   â””â”€â–º Save training_results.json
   â†“
6. Evaluate Task
   â”œâ”€â–º Load checkpoint
   â”œâ”€â–º Test on val set
   â””â”€â–º Calculate metrics
   â†“
7. Deploy Task
   if new_val_loss < current_val_loss:
       â”œâ”€â–º Backup current model
       â”œâ”€â–º Copy new model â†’ /opt/backend/models/checkpoints/
       â””â”€â–º Update config.json
   else:
       â””â”€â–º Skip deployment
   â†“
8. Notify Task
   â””â”€â–º Log completion status
```

---

## âœ… Checklist

Before running in production:

- [ ] Airflow services started successfully
- [ ] Can access Airflow UI (http://localhost:8080)
- [ ] Both DAGs appear in UI
- [ ] PySpark service is running (http://localhost:5001)
- [ ] MongoDB is accessible
- [ ] Training script created: `Stage2/train_model.py`
- [ ] Test DAG manually first
- [ ] Check logs for errors
- [ ] Verify model deployment works

---

## ğŸ¯ Next Steps

### 1. Create Training Script

Create `Stage2/train_model.py` with actual training logic:

```python
# Example structure
import torch
from transformers import AutoTokenizer, AutoModel
# ... import your model

def main():
    # Load data
    with open('train_data.json') as f:
        train_data = json.load(f)

    # Initialize model
    model = MultiEAOSModel(...)

    # Training loop
    for epoch in range(num_epochs):
        # Train on Spark
        # ...

    # Save checkpoint
    torch.save({
        'model_state_dict': model.state_dict(),
        'epoch': epoch,
        'val_loss': val_loss
    }, 'latest_checkpoint.pth')

if __name__ == '__main__':
    main()
```

### 2. Test Model Retraining

```bash
# Trigger manually
docker exec -it tv-analytics-airflow-scheduler \
    airflow dags trigger model_retraining

# Watch progress in UI
# http://localhost:8080
```

### 3. Monitor Logs

```bash
# Follow scheduler logs
docker compose logs -f airflow-scheduler

# Check specific task
docker exec -it tv-analytics-airflow-scheduler \
    airflow tasks logs model_retraining train_model <run_id>
```

---

## ğŸ“š Resources

- **Airflow Docs**: https://airflow.apache.org/docs/
- **Cron Expression**: https://crontab.guru/
- **DAG Best Practices**: https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html

---

**ğŸ‰ Airflow Setup Complete!**

Giá» báº¡n cÃ³ thá»ƒ:
- âœ… Schedule model retraining tá»± Ä‘á»™ng
- âœ… Trigger training manually khi cáº§n
- âœ… Monitor pipeline execution
- âœ… View detailed logs
- âœ… Retry failed tasks
- âœ… Deploy models automatically
